{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLHr_lZuYGcD"
      },
      "source": [
        "## About\n",
        "\n",
        "Function Gemma ->  .litertlm Conversion\n",
        "\n",
        "\n",
        "Converts fine-tuned FunctionGemma model to `.litertlm` format for LiteRT-LM runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5iS0A71YGcG"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install ai-edge-torch-nightly for model conversion to .litertlm format.\n",
        "\n",
        "**Important:**\n",
        "- We use nightly builds (API may change)\n",
        "- numpy<2.1 is required for compatibility\n",
        "- **RESTART RUNTIME** after this step!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GiOPicuUYGcH",
        "outputId": "46edc2ec-ef48-4db5-8e76-081ab8c12acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m316.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m225.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.4/459.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.0/569.0 MB\u001b[0m \u001b[31m190.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m165.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m289.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m202.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m306.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m161.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m255.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m305.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m231.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m278.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m222.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m170.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m251.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m288.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m155.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m290.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m166.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m311.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m297.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m311.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m309.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m273.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m309.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m179.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m273.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m268.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m330.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m349.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m289.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m246.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m311.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m355.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m429.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m279.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m376.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m319.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m277.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m274.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m421.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m367.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m354.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m263.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m410.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m343.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m402.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m372.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m378.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m376.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m322.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m373.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m445.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m422.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m169.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m367.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m398.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m302.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.2/408.2 kB\u001b[0m \u001b[31m373.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m330.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m440.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m344.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
            "bigframes 2.31.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "bigframes 2.31.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.1.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "bigframes 2.31.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installed:\n",
            "Version: 0.8.0.dev20260108\n",
            "Version: 4.57.3\n",
            "Version: 2.0.2\n",
            "Version 3.1, 31 March 2009\n",
            "                       Version 3, 29 June 2007\n",
            "  5. Conveying Modified Source Versions.\n",
            "  14. Revised Versions of this License.\n",
            "Version: 12.1.0\n",
            "\n",
            "⚠️  RESTART RUNTIME after this step! (Runtime → Restart session)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 1: Install ai-edge-torch-nightly\n",
        "# =============================================================================\n",
        "!pip uninstall -y tensorflow 2>/dev/null || true\n",
        "!pip cache purge\n",
        "\n",
        "# Install ai-edge-torch packages\n",
        "!pip install ai-edge-torch-nightly --force-reinstall --no-cache-dir -q\n",
        "!pip install ai-edge-litert-nightly --no-cache-dir -q\n",
        "\n",
        "# CRITICAL: Install numpy<2.1 AFTER ai-edge-torch (it may override)\n",
        "!pip install \"numpy<2.1\" --force-reinstall -q\n",
        "\n",
        "# Install transformers with pinned version\n",
        "!pip install transformers==4.57.3 huggingface_hub sentencepiece -q\n",
        "\n",
        "# Restore Colab's native Pillow\n",
        "!pip install Pillow --force-reinstall -q\n",
        "\n",
        "print(\"\\nInstalled:\")\n",
        "!pip show ai-edge-torch-nightly | grep Version\n",
        "!pip show transformers | grep Version\n",
        "!pip show numpy | grep Version\n",
        "!pip show Pillow | grep Version\n",
        "\n",
        "print(\"\\n⚠️  RESTART RUNTIME after this step! (Runtime → Restart session)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "C5IBIJNaYacP",
        "outputId": "9b5bda4f-f8d0-49af-c1f4-d8314fe42dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZHJbdGgYGcJ"
      },
      "source": [
        "## Step 2: Load Model Tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-qXrXINEYGcK",
        "outputId": "efebb46c-05fd-4cd4-8aa9-aa078e234634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found folder: /content/drive/MyDrive/model-tuned-final\n",
            "\n",
            "Model ready:\n",
            "total 562076\n",
            "drwx------ 2 root root      4096 Jan  8 21:03 .\n",
            "drwxr-xr-x 1 root root      4096 Jan  8 21:03 ..\n",
            "-rw------- 1 root root        67 Jan  8 21:08 added_tokens.json\n",
            "-rw------- 1 root root     14071 Jan  8 21:08 chat_template.jinja\n",
            "-rw------- 1 root root      1395 Jan  8 21:08 config.json\n",
            "-rw------- 1 root root       240 Jan  8 21:08 generation_config.json\n",
            "-rw------- 1 root root 536223056 Jan  8 21:08 model.safetensors\n",
            "-rw------- 1 root root       740 Jan  8 21:08 special_tokens_map.json\n",
            "-rw------- 1 root root   1207069 Jan  8 21:08 tokenizer_config.json\n",
            "-rw------- 1 root root  33384899 Jan  8 21:08 tokenizer.json\n",
            "-rw------- 1 root root   4689144 Jan  8 21:08 tokenizer.model\n",
            "-rw------- 1 root root      5816 Jan  8 21:08 training_args.bin\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 2: Load fine-tuned model from Google Drive\n",
        "# =============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_NAME = \"model-tuned-final\"\n",
        "MODEL_DIR = MODEL_NAME\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{MODEL_NAME}\"\n",
        "DRIVE_ZIP = f\"/content/drive/MyDrive/{MODEL_NAME}.zip\"\n",
        "\n",
        "if os.path.exists(DRIVE_MODEL_DIR):\n",
        "    print(f\"Found folder: {DRIVE_MODEL_DIR}\")\n",
        "    !cp -r \"{DRIVE_MODEL_DIR}\" .\n",
        "elif os.path.exists(DRIVE_ZIP):\n",
        "    print(f\"Found ZIP: {DRIVE_ZIP}\")\n",
        "    !unzip -q \"{DRIVE_ZIP}\"\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model not found!\\nUpload to: {DRIVE_MODEL_DIR}/ or {DRIVE_ZIP}\")\n",
        "\n",
        "print(f\"\\nModel ready:\")\n",
        "!ls -la \"{MODEL_DIR}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCkP6XGFYGcL"
      },
      "source": [
        "## Step 3: Test Model Before Conversion\n",
        "\n",
        "**CRITICAL**: Verify the model works BEFORE converting to litertlm.\n",
        "If it outputs garbage here, the problem is in weight loading, not conversion."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import json\n",
        "\n",
        "# FunctionGemma special tokens\n",
        "START_TURN = \"\"\n",
        "END_TURN = \"\"\n",
        "START_DECL = \"\"\n",
        "END_DECL = \"\"\n",
        "START_CALL = \"\"\n",
        "END_CALL = \"\"\n",
        "ESCAPE = \"\"\n",
        "\n",
        "FUNCTION_DECLARATIONS = f\"\"\"{START_DECL}declaration:pagamento{{description:{ESCAPE}Realiza um pagamento via Pix na maquininha e opcionalmente imprime o comprovante{ESCAPE},parameters:{{properties:{{valor:{{description:{ESCAPE}Valor do pagamento em reais (BRL){ESCAPE},type:{ESCAPE}NUMBER{ESCAPE}}},nome_estabelecimento:{{description:{ESCAPE}Nome exibido na maquininha{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}},imprimir:{{description:{ESCAPE}Indica se deve imprimir o comprovante{ESCAPE},type:{ESCAPE}BOOLEAN{ESCAPE}}}}},required:[{ESCAPE}valor{ESCAPE},{ESCAPE}nome_estabelecimento{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\"\"\"\n",
        "\n",
        "print(f\"Loading model from {MODEL_DIR}...\")\n",
        "\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "hf_model.eval()\n",
        "print(f\"Model loaded on {hf_model.device}, dtype={hf_model.dtype}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "\n",
        "test_prompt = f\"\"\"{START_TURN}developer\n",
        "Você é um modelo especialista em chamada de funções para pagamentos via Pix.\n",
        "{FUNCTION_DECLARATIONS}\n",
        "{END_TURN}\n",
        "{START_TURN}user\n",
        "quero pagar 50 reais na padaria\n",
        "{END_TURN}\n",
        "{START_TURN}model\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TESTING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Input: 'quero pagar 50 reais na padaria'\")\n",
        "\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(hf_model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = hf_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\n",
        "print(f\"\\nModel output:\\n{response}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "if \"pagamento\" in response or \"call:\" in response:\n",
        "    print(\"✅ Fine-tuned model outputs function call - GOOD!\")\n",
        "elif \"<pad>\" in response[:50]:\n",
        "    print(\"❌ Model outputs <pad> - wrong loading parameters!\")\n",
        "    raise ValueError(\"STOP: Wrong model loading parameters\")\n",
        "elif \"apologize\" in response.lower() or \"sorry\" in response.lower():\n",
        "    print(\"❌ Model refuses to call function - fine-tuning didn't work!\")\n",
        "    raise ValueError(\"STOP: Model not fine-tuned correctly\")\n",
        "elif any(c in response for c in \"为足球收消气\"):\n",
        "    print(\"❌ Model outputs garbage - fine-tuning is broken!\")\n",
        "    raise ValueError(\"STOP: Model outputs garbage\")\n",
        "else:\n",
        "    print(\"⚠️ Unexpected output - review manually\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\nModel unloaded, ready for ai-edge-torch conversion.\")\n"
      ],
      "metadata": {
        "id": "wBlEjqgQYGcM",
        "outputId": "d5bb5b57-c13a-499b-f8bf-bce2c2a06e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from model-tuned-final...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.8.2, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cpu, dtype=torch.bfloat16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from 'model-tuned-final' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TESTING FINE-TUNED MODEL\n",
            "==================================================\n",
            "Input: 'quero pagar 50 reais na padaria'\n",
            "\n",
            "Model output:\n",
            "call:pagamento{valor:50,nome_estabelecimento:Padaria Boa Massa,imprimir:True}<eos>\n",
            "==================================================\n",
            "✅ Fine-tuned model outputs function call - GOOD!\n",
            "\n",
            "Model unloaded, ready for ai-edge-torch conversion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Convert to .litertlm\n",
        "\n",
        "If the test above shows garbage output, **STOP HERE** - the problem is in `gemma3.build_model_270m()` not loading weights correctly."
      ],
      "metadata": {
        "id": "ddSt5SbrYGcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0JyiXmyEYGcO",
        "outputId": "e4c1f00d-349a-467c-95e1-50f1c144120b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:351: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
            "  warnings.warn(\n",
            "ERROR:2026-01-08 21:12:45,403:jax._src.xla_bridge:475: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/xla_bridge.py\", line 473, in discover_pjrt_plugins\n",
            "    plugin_module.initialize()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 348, in initialize\n",
            "    xla_client.register_custom_type_id_handler(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'jaxlib.xla_client' has no attribute 'register_custom_type_id_handler'. Did you mean: 'register_custom_type_handler'?\n",
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/xla_bridge.py\", line 473, in discover_pjrt_plugins\n",
            "    plugin_module.initialize()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 348, in initialize\n",
            "    xla_client.register_custom_type_id_handler(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: module 'jaxlib.xla_client' has no attribute 'register_custom_type_id_handler'. Did you mean: 'register_custom_type_handler'?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from model-tuned-final via ai-edge-torch...\n",
            "Model loaded!\n",
            "Tokenizer: model-tuned-final/tokenizer.model\n",
            "Metadata created: litertlm_output/base_llm_metadata.textproto\n",
            "\n",
            "==================================================\n",
            "Converting to .litertlm...\n",
            "Time: ~5-15 min (A100)\n",
            "==================================================\n",
            "\n",
            ".litertlm conversion complete!\n",
            "\n",
            "Generated files:\n",
            "total 272M\n",
            "drwxr-xr-x 2 root root 4.0K Jan  8 21:20 .\n",
            "drwxr-xr-x 1 root root 4.0K Jan  8 21:12 ..\n",
            "-rw-r--r-- 1 root root  210 Jan  8 21:12 base_llm_metadata.textproto\n",
            "-rw-r--r-- 1 root root 272M Jan  8 21:20 functiongemma-litertlm_q8_ekv1024.litertlm\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Step 4: Convert to .litertlm format (using official Google parameters)\n",
        "# Source: https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n",
        "# =============================================================================\n",
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "\n",
        "# Load model using ai-edge-torch (required for conversion)\n",
        "print(f\"Loading model from {MODEL_DIR} via ai-edge-torch...\")\n",
        "pytorch_model = gemma3.build_model_270m(MODEL_DIR)\n",
        "pytorch_model.eval()\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "LITERTLM_OUTPUT_DIR = \"litertlm_output\"\n",
        "os.makedirs(LITERTLM_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "export_config = ExportConfig()\n",
        "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "export_config.mask_as_input = True\n",
        "\n",
        "# Find tokenizer\n",
        "TOKENIZER_PATH = f\"{MODEL_DIR}/tokenizer.model\"\n",
        "print(f\"Tokenizer: {TOKENIZER_PATH}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Create FunctionGemma metadata (OFFICIAL Google format)\n",
        "# Only 2 stop tokens as per official cookbook\n",
        "# =============================================================================\n",
        "METADATA_PATH = f\"{LITERTLM_OUTPUT_DIR}/base_llm_metadata.textproto\"\n",
        "\n",
        "metadata_content = r\"\"\"start_token: {\n",
        "    token_ids: {\n",
        "        ids: [ 2 ]\n",
        "    }\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<end_of_turn>\"\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<start_function_response>\"\n",
        "}\n",
        "llm_model_type: {\n",
        "    function_gemma: {}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open(METADATA_PATH, 'w') as f:\n",
        "    f.write(metadata_content)\n",
        "print(f\"Metadata created: {METADATA_PATH}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Converting to .litertlm...\")\n",
        "print(\"Time: ~5-15 min (A100)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Convert with OFFICIAL Google parameters\n",
        "# Source: gemma-cookbook/FunctionGemma/Finetune_FunctionGemma_270M_for_Mobile_Actions\n",
        "try:\n",
        "    converter.convert_to_litert(\n",
        "        pytorch_model,\n",
        "        output_path=LITERTLM_OUTPUT_DIR,\n",
        "        output_name_prefix=\"functiongemma-litertlm\",\n",
        "        prefill_seq_len=256,           # Official: 256 (NOT 2048!)\n",
        "        kv_cache_max_len=1024,         # Official: 1024 (NOT 4096!)\n",
        "        quantize=\"dynamic_int8\",\n",
        "        export_config=export_config,\n",
        "        output_format=\"litertlm\",\n",
        "        tokenizer_model_path=TOKENIZER_PATH,\n",
        "        base_llm_metadata_path=METADATA_PATH,  # CRITICAL: base_llm_metadata_path, NOT llm_metadata_path!\n",
        "    )\n",
        "    print(\"\\n.litertlm conversion complete!\")\n",
        "except (TypeError, AttributeError) as e:\n",
        "    print(f\"\\nlitertlm not supported in this version: {e}\")\n",
        "    print(\"Falling back to .tflite...\")\n",
        "    converter.convert_to_tflite(\n",
        "        pytorch_model,\n",
        "        output_path=LITERTLM_OUTPUT_DIR,\n",
        "        output_name_prefix=\"functiongemma-litertlm\",\n",
        "        prefill_seq_len=256,\n",
        "        kv_cache_max_len=1024,\n",
        "        quantize=\"dynamic_int8\",\n",
        "        export_config=export_config,\n",
        "    )\n",
        "    print(\"\\n.tflite conversion complete\")\n",
        "\n",
        "print(\"\\nGenerated files:\")\n",
        "!ls -lah {LITERTLM_OUTPUT_DIR}/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJcceTSsYGcP"
      },
      "source": [
        "## Step 5: Save and Download\n",
        "\n",
        "Save the ready `.litertlm` file:\n",
        "1. To Google Drive — for future use\n",
        "2. Download locally — to use with LiteRT-LM runtime\n",
        "\n",
        "After downloading, you can use the model with:\n",
        "- [CLI tool `lit`](https://github.com/google-ai-edge/LiteRT-LM/releases)\n",
        "- Kotlin API for Android/JVM\n",
        "- C++ API for native integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZQSe8LPYGcP"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Step 5: Save to Google Drive and download\n",
        "# =============================================================================\n",
        "import glob\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Find output files\n",
        "output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.litertlm\")\n",
        "if not output_files:\n",
        "    output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.tflite\")\n",
        "\n",
        "if not output_files:\n",
        "    raise FileNotFoundError(\"No output files found!\")\n",
        "\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Saving to Google Drive:\")\n",
        "for f in output_files:\n",
        "    size = os.path.getsize(f) / 1e6\n",
        "    filename = os.path.basename(f)\n",
        "    drive_path = f\"{DRIVE_OUTPUT_DIR}/{filename}\"\n",
        "    shutil.copy(f, drive_path)\n",
        "    print(f\"  {filename} ({size:.1f} MB) -> {drive_path}\")\n",
        "\n",
        "print(\"\\nDownloading:\")\n",
        "for f in output_files:\n",
        "    files.download(f)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"DONE!\")\n",
        "print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}